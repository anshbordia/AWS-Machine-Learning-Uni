{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 3</a>\n",
    "\n",
    "## Final Project: Neural Networks and Recurrent Neural Networks (RNNs) for the IMDB Movie Review Dataset\n",
    "\n",
    "__Dataset:__ Sentiment (positive or negative) analysis of movie reviews. The dataset is originally hosted here: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "We continue to work on our final project dataset. This time, you will try to see how Neural Networks, Recurrent Neural Networks (RNNs), its variants: GRU and LSTM work in predicting the sentiment of review texts. If you are interested in trying Transformers, here is a good place for that too!\n",
    "\n",
    "Use the notebooks from the class and implement the model, train and test with the corresponding datasets.\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a classifier (Implement)\n",
    "3. Make predictions on your test dataset (Implement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie makes me want to throw up every tim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Listening to the director's commentary confirm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the best Tarzan films is also one of it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Valentine is now one of my favorite slasher fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No mention if Ann Rivers Siddons adapted the m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  This movie makes me want to throw up every tim...      0\n",
       "1  Listening to the director's commentary confirm...      0\n",
       "2  One of the best Tarzan films is also one of it...      1\n",
       "3  Valentine is now one of my favorite slasher fi...      1\n",
       "4  No mention if Ann Rivers Siddons adapted the m...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/final_project/imdb_train.csv', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What I hoped for (or even expected) was the we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Garden State must rate amongst the most contri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is a lot wrong with this film. I will no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To qualify my use of \"realistic\" in the summar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty War is absolutely one of the best politi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  What I hoped for (or even expected) was the we...      0\n",
       "1  Garden State must rate amongst the most contri...      0\n",
       "2  There is a lot wrong with this film. I will no...      1\n",
       "3  To qualify my use of \"realistic\" in the summar...      1\n",
       "4  Dirty War is absolutely one of the best politi...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('../data/final_project/imdb_test.csv', header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n",
      "0    12500\n",
      "1    12500\n",
      "Name: label, dtype: int64\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Exlore distribution of classes\n",
    "print(train_df['label'].value_counts())\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "#Explore NaNs\n",
    "print(train_df.isnull().values.any())\n",
    "print(test_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anshbordia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anshbordia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anshbordia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download relevant nltk packages\n",
    "import nltk, re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words removal\n",
    "#Not all stop words are bad in this scenario. We will retain some helpful words as shown below\n",
    "from nltk.corpus import stopwords\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
    "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "accepted_stopwords = list(set(stopwords.words('english')).symmetric_difference(set(excluding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup tokenizer and lemmatizer for preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert to lower case\n",
    "Remove trailing white space\n",
    "Remove intermediate extra white space\n",
    "Remove HTML tags\n",
    "Lemmatize individual words\n",
    "\"\"\"\n",
    "def preprocess(text):\n",
    "    processed_sentences = []\n",
    "    temp = \"\"\n",
    "    for i in range(0, len(text)):\n",
    "        temp = text[i]\n",
    "        temp = temp.lower()\n",
    "        temp = temp.strip()\n",
    "        temp = re.sub('\\s+', ' ', temp)\n",
    "        temp = re.compile('<.*?>').sub('', temp)\n",
    "        \n",
    "        selected_words = []\n",
    "        for word in word_tokenize(temp):\n",
    "            if(word not in accepted_stopwords and not word.isnumeric()):\n",
    "                selected_words.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "        processed_sentences.append(\" \".join(selected_words))\n",
    "        temp = \"\"\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = preprocess(train_df['text'].values)\n",
    "test_X = preprocess(test_df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['label'].values\n",
    "test_y = test_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT sentence transformer for generating sentence embeddings\n",
    "#Takes too long, so doing for only 1000\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "embedded_sentences = bert.encode(train_X[0:1000])\n",
    "embedded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to 2D array for input into LSTM\n",
    "embedded_sentences = np.expand_dims(embedded_sentences, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1, 768)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 128)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,393\n",
      "Trainable params: 459,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 4s 4ms/step - loss: 0.6337 - accuracy: 0.6213\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4941 - accuracy: 0.7501\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4756 - accuracy: 0.7712\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4463 - accuracy: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc063b2b6a0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM Training\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, input_shape = (1,768)))\n",
    "lstm.add(Dropout(0.25))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm.summary())\n",
    "lstm.fit(embedded_sentences, train_y[0:1000], epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on 500 reviews\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_embeds = bert.encode(test_X[0:500])\n",
    "test_embeds = np.expand_dims(test_embeds, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       263\n",
      "           1       0.76      0.76      0.76       237\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.77      0.77      0.77       500\n",
      "weighted avg       0.77      0.77      0.77       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshbordia/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "preds = lstm.predict_classes(test_embeds)\n",
    "print(classification_report(test_y[0:500], preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
